{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import psycopg2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sql import create_table_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(filepath):\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        files = glob.glob(os.path.join(root,'*.json'))\n",
    "        for f in files :\n",
    "            all_files.append(os.path.abspath(f))\n",
    "    \n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup DB connection and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_config = \"host=postgres-db dbname=udacity user=udacity password=udacity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(db_config)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in create_table_queries:\n",
    "    cur.execute(query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"SELECT table_name FROM information_schema.tables\n",
    "       WHERE table_schema = 'public'\"\"\")\n",
    "for table in cur.fetchall():\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process `song_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we set a list of all song data filepaths, and inspect one of the json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_files = get_files('./data/song_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file_path = song_files[0]\n",
    "df = pd.DataFrame(pd.read_json(sample_file_path, typ='series', convert_dates=False))\n",
    "df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a combined dataframe with all the log data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for file in song_files:\n",
    "    dfs.append(pd.DataFrame([pd.read_json(file, typ='series', convert_dates=False)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we set artist_id as index and deduplicate, create a list of tuples to be inserted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_data_list = []\n",
    "result = pd.concat(dfs)\n",
    "result = result.reset_index(drop=True)\n",
    "result = result.set_index('artist_id',  drop=False)\n",
    "result = result[~result.index.duplicated(keep='first')]\n",
    "for value in result.values:\n",
    "        num_songs, artist_id, artist_latitude, artist_longitude, artist_location, artist_name, song_id, title, duration, year = value\n",
    "        artist_data = (artist_id, artist_name, artist_location, artist_latitude, artist_longitude)\n",
    "        artist_data_list.append(artist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(db_config)\n",
    "cur = conn.cursor()\n",
    "args_str = ','.join(cur.mogrify(\"(%s,%s,%s,%s,%s)\", x).decode(\"utf-8\") for x in artist_data_list)\n",
    "cur.execute(\"INSERT INTO artists VALUES \" + args_str)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(db_config)\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT * FROM artists\")\n",
    "artist_records = cur.fetchall()\n",
    "artist_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we get all the song data. But this time we don't need to deduplicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data_list = []\n",
    "result = pd.concat(dfs)\n",
    "for value in result.values:\n",
    "        num_songs, artist_id, artist_latitude, artist_longitude, artist_location, artist_name, song_id, title, duration, year = value\n",
    "        song_data = (song_id, title, artist_id, year, duration)\n",
    "        song_data_list.append(song_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(db_config)\n",
    "cur = conn.cursor()\n",
    "args_str = ','.join(cur.mogrify(\"(%s,%s,%s,%s,%s)\", x).decode(\"utf-8\") for x in song_data_list)\n",
    "cur.execute(\"INSERT INTO songs VALUES \" + args_str)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(db_config)\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT * FROM songs\")\n",
    "song_records = cur.fetchall()\n",
    "song_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process log data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you'll perform ETL on the second dataset, log_data, to create the time and users dimensional tables, as well as the songplays fact table.\n",
    "\n",
    "Let's perform ETL on a single log file and load a single record into each table.\n",
    "\n",
    "Use the get_files function provided above to get a list of all log JSON files in data/log_data\n",
    "Select the first log file in this list\n",
    "Read the log file and view the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_files = get_files('./data/log_data')\n",
    "sample_file_path = log_files[0]\n",
    "\n",
    "df = pd.DataFrame(pd.read_json(sample_file_path, typ='frame', convert_dates=False, lines=True))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Data for Time Table\n",
    "\n",
    "Filter records by NextSong action\n",
    "\n",
    "Convert the ts timestamp column to datetime\n",
    "\n",
    "Hint: the current timestamp is in milliseconds\n",
    "\n",
    "Extract the timestamp, hour, day, week of year, month, year, and weekday from the ts column and set time_data to a list containing these values in order\n",
    "\n",
    "Hint: use pandas' dt attribute to access easily datetimelike properties.\n",
    "\n",
    "Specify labels for these columns and set to column_labels\n",
    "\n",
    "Create a dataframe, time_df, containing the time data for this file by combining column_labels and time_data into a dictionary and converting this into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dfs = []\n",
    "for file in log_files:\n",
    "    log_dfs.append(pd.DataFrame(pd.read_json(file, typ='frame', convert_dates=False, lines=True)))\n",
    "log_result = pd.concat(log_dfs)\n",
    "log_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_page = \"NextSong\"\n",
    "next_song_df = log_result.loc[log_result['page'] == required_page]\n",
    "next_song_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_song_df_w_datetime = next_song_df.copy()\n",
    "next_song_df_w_datetime.loc[:, 'datetime'] = pd.to_datetime(next_song_df['ts'], unit='ms')\n",
    "next_song_df_w_datetime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df = next_song_df_w_datetime[['datetime']].copy()\n",
    "ts_df.loc[:,'hour'] = ts_df['datetime'].dt.hour\n",
    "ts_df.loc[:,'day'] = ts_df['datetime'].dt.day\n",
    "ts_df.loc[:,'week'] = ts_df['datetime'].dt.isocalendar().week\n",
    "ts_df.loc[:,'month'] = ts_df['datetime'].dt.month\n",
    "ts_df.loc[:,'year'] = ts_df['datetime'].dt.year\n",
    "ts_df.loc[:,'weekday'] = ts_df['datetime'].dt.weekday\n",
    "ts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_table_insert = 'INSERT INTO time VALUES (%s, %s, %s, %s, %s, %s, %s) ON CONFLICT (start_time) DO NOTHING'\n",
    "conn = psycopg2.connect(db_config)\n",
    "cur = conn.cursor()\n",
    "for i, row in ts_df.iterrows():\n",
    "    cur.execute(time_table_insert, list(row))\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### users Table\n",
    "Extract Data for Users Table\n",
    "\n",
    "1.Select columns for user ID, first name, last name, gender and level and set to user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = log_result[['userId','firstName','lastName','gender','level']].copy()\n",
    "user_df = user_df.loc[user_df['userId'] != '']\n",
    "user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_table_insert = '''INSERT INTO users (user_id, first_name, last_name, gender, level) VALUES (%s, %s, %s, %s, %s) \n",
    "                        ON CONFLICT (user_id) DO UPDATE SET \n",
    "                        level = EXCLUDED.level'''\n",
    "conn = psycopg2.connect(db_config)\n",
    "cur = conn.cursor()\n",
    "for i, row in user_df.iterrows():\n",
    "    cur.execute(user_table_insert, list(row))\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### songplays table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Data and Songplays Table\n",
    "\n",
    "This one is a little more complicated since information from the songs table, artists table, and original log file are all needed for the songplays table. Since the log file does not specify an ID for either the song or the artist, you'll need to get the song ID and artist ID by querying the songs and artists tables to find matches based on song title, artist name, and song duration time.\n",
    "\n",
    "Implement the song_select query in sql_queries.py to find the song ID and artist ID based on the title, artist name, and duration of a song.\n",
    "\n",
    "Select the timestamp, user ID, level, song ID, artist ID, session ID, location, and user agent and set to songplay_data\n",
    "\n",
    "Insert Records into Songplays Table\n",
    "\n",
    "Implement the songplay_table_insert query and run the cell below to insert records for the songplay actions in this log file into the songplays table. Remember to run create_tables.py before running the cell below to ensure you've created/resetted the songplays table in the sparkify database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_select = '''SELECT song_id, artists.artist_id\n",
    "    FROM songs JOIN artists ON songs.artist_id = artists.artist_id\n",
    "    WHERE songs.title = %s\n",
    "    AND artists.name = %s\n",
    "    AND songs.duration = %s\n",
    "'''\n",
    "songplay_table_insert = '''\n",
    "INSERT INTO songplays VALUES (DEFAULT, %s, %s, %s, %s, %s, %s, %s, %s )\n",
    "'''\n",
    "conn = psycopg2.connect(db_config)\n",
    "cur = conn.cursor()\n",
    "for index, row in next_song_df_w_datetime.iterrows():\n",
    "    cur.execute(song_select, (row.song, row.artist, row.length))\n",
    "    results = cur.fetchone()\n",
    "    \n",
    "    if results:\n",
    "        songid, artistid = results\n",
    "    else:\n",
    "        songid, artistid = None, None\n",
    "\n",
    "    songplay_data = (row.datetime, row.userId, row.level, songid, artistid, row.sessionId, row.location, row.userAgent)\n",
    "    cur.execute(songplay_table_insert, songplay_data)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
